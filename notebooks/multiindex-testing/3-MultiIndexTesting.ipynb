{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Index Testing with graphrag library API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import query api from graphrag library\n",
    "# Make sure to install graphrag version 0.3.0 to be able to get these packages\n",
    "from graphrag.query.api import global_search, local_search\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "import yaml\n",
    "from graphrag.config import create_graphrag_config\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom pipeline settings m\n",
    "this_directory = os.path.dirname(\n",
    "            os.path.abspath(inspect.getfile(inspect.currentframe()))\n",
    "        )\n",
    "data = yaml.safe_load(open(f\"./pipeline-settings.yaml\"))\n",
    "# layer the custom settings on top of the default configuration settings of graphrag\n",
    "parameters = create_graphrag_config(data, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logic to iterate through indexes, convert parquets to dataframes, append dataframes to a list, then concat them together to pass to graphrag.query.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index folder names\n",
    "\n",
    "index_names = ['./arizona', './alaska']\n",
    "\n",
    "nodes_dfs =[]\n",
    "community_dfs =[]\n",
    "entities_dfs = []\n",
    "text_units_dfs=[]\n",
    "relationships_dfs = []\n",
    "covariates_dfs = []\n",
    "\n",
    "links = {\"nodes\": {}, \"community\": {}, \"entities\": {}, \"text_units\": {}, \"relationships\": {}, \"covariates\": {}}\n",
    "max_vals = {\"nodes\": -1, \"community\": -1, \"entities\": -1, \"text_units\": -1, \"relationships\": -1, \"covariates\": -1}\n",
    "\n",
    "for index_name in index_names:\n",
    "    # Construct the path to the output directory\n",
    "    output_path = os.path.join(index_name, \"output\")\n",
    "    # Get all subdirectories in the output folder\n",
    "    subdirs = [os.path.join(output_path, d) for d in os.listdir(output_path) if os.path.isdir(os.path.join(output_path, d))]\n",
    "    \n",
    "    # Find the most recently created subdirectory\n",
    "    most_recent_subdir = max(subdirs, key=os.path.getmtime)\n",
    "    \n",
    "    # Construct the path to the respective parquet files, takes the most recent index iteration in the artifacts folder\n",
    "    nodes_file_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_nodes.parquet\")\n",
    "    community_report_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_community_reports.parquet\")  # Adjust the filename/path as necessary\n",
    "    entities_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_entities.parquet\")  # Adjust the filename/path as necessary\n",
    "    text_units_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_text_units.parquet\")\n",
    "    relationships_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_relationships.parquet\")\n",
    "    covariates_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_covariates.parquet\")\n",
    "    # Read the parquet file into a DataFrame\n",
    "    #For community df, id column is \"community\" and id type id \"str\". Convert type from \"str\" to \"int\", add max_id to the int, convert back to string and overwrite the column\n",
    "    #For entities df, id column is \"human_readable_id\" and id type id \"int\". Add max_id to the int and overwrite the column. Add index name to the name column and overwrite the column,\n",
    "    #add index name to the text_unit_ids column and overwrite the column\n",
    "    #For nodes df, id column is \"human_readable_id\" and id type id \"int\". Add max_id to the int and overwrite the column. Add index name to the title column and overwrite the column\n",
    "    #add index name to the source_id column and overwrite the column, add max_id for community to the community column and overwrite the column\n",
    "    #For text units df, id column is \"id\" and id type is \"str\". Append index name with hyphen to the id and overwrite the column\n",
    "    #For relationships df, id column is \"human_readable_id\" and id type is \"str\". Convert type from \"str\" to \"int\", add max_id to the int, convert back to string and overwrite the column,\n",
    "    #add index name to the source column and overwrite the column, add index name to the target column and overwrite the column\n",
    "    #For covariates df, id column is \"human_readable_id\" and id type is \"str\". Convert type from \"str\" to \"int\", add max_id to the int, convert back to string and overwrite the column\n",
    "    \n",
    "    #Note that nodes need to set before communities to that max community id makes sense\n",
    "    if os.path.exists(nodes_file_path):\n",
    "        nodes_df = pd.read_parquet(nodes_file_path)\n",
    "        for i in nodes_df[\"human_readable_id\"]:\n",
    "            links[\"nodes\"][i + max_vals[\"nodes\"] + 1] = {\"index_name\": index_name, \"id\": i}\n",
    "        if max_vals[\"nodes\"] != -1:\n",
    "           nodes_df[\"human_readable_id\"] += max_vals[\"nodes\"] + 1\n",
    "        nodes_df[\"community\"] = nodes_df[\"community\"].apply(lambda x: str(int(x) + max_vals[\"community\"] +1) if x else x)\n",
    "        nodes_df[\"title\"] = nodes_df[\"title\"].apply(lambda x: x + f\"-{index_name}\")\n",
    "        nodes_df[\"source_id\"] = nodes_df[\"source_id\"].apply(lambda x: \",\".join([i + f\"-{index_name}\" for i in x.split(\",\")]))\n",
    "        max_vals[\"nodes\"] = nodes_df[\"human_readable_id\"].max() \n",
    "        nodes_dfs.append(nodes_df)\n",
    "    else:\n",
    "        print(f\"File {nodes_file_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(community_report_table_path):\n",
    "        community_df = pd.read_parquet(community_report_table_path)\n",
    "        for i in community_df[\"community\"].astype(int):\n",
    "            links[\"community\"][i + max_vals[\"community\"] + 1] = {\"index_name\": index_name, \"id\": str(i)}\n",
    "        if max_vals[\"community\"] != -1:\n",
    "            col = community_df[\"community\"].astype(int) + max_vals[\"community\"] + 1\n",
    "            community_df[\"community\"] = col.astype(str)\n",
    "        max_vals[\"community\"] = community_df[\"community\"].astype(int).max()\n",
    "        community_dfs.append(community_df)\n",
    "    else:\n",
    "        print(f\"File {community_report_table_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(entities_table_path):\n",
    "        entities_df= pd.read_parquet(entities_table_path)\n",
    "        for i in entities_df[\"human_readable_id\"]:\n",
    "            links[\"entities\"][i + max_vals[\"entities\"] + 1] = {\"index_name\": index_name, \"id\": i}\n",
    "        if max_vals[\"entities\"] != -1:\n",
    "           entities_df[\"human_readable_id\"] += max_vals[\"entities\"] + 1\n",
    "        entities_df[\"name\"] = entities_df[\"name\"].apply(lambda x: x + f\"-{index_name}\")\n",
    "        entities_df[\"text_unit_ids\"] = entities_df[\"text_unit_ids\"].apply(lambda x: [i + f\"-{index_name}\" for i in x])\n",
    "        max_vals[\"entities\"] = entities_df[\"human_readable_id\"].max()\n",
    "        entities_dfs.append(entities_df)\n",
    "    else:\n",
    "        print(f\"File {entities_table_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(text_units_table_path):\n",
    "        text_units_df= pd.read_parquet(text_units_table_path)\n",
    "        text_units_df[\"id\"] = text_units_df[\"id\"].apply(lambda x: f\"{x}-{index_name}\")\n",
    "        text_units_dfs.append(text_units_df)\n",
    "    else:\n",
    "        print(f\"File {text_units_table_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(relationships_table_path):\n",
    "        relationships_df= pd.read_parquet(relationships_table_path)\n",
    "        for i in relationships_df[\"human_readable_id\"].astype(int):\n",
    "            links[\"relationships\"][i + max_vals[\"relationships\"] + 1] = {\"index_name\": index_name, \"id\": i}\n",
    "        if max_vals[\"relationships\"] != -1:\n",
    "            col = relationships_df[\"human_readable_id\"].astype(int) + max_vals[\"relationships\"] + 1\n",
    "            relationships_df[\"human_readable_id\"] = col.astype(str)\n",
    "        relationships_df[\"source\"] = relationships_df[\"source\"].apply(lambda x: x + f\"-{index_name}\")\n",
    "        relationships_df[\"target\"] = relationships_df[\"target\"].apply(lambda x: x + f\"-{index_name}\")\n",
    "        relationships_df[\"text_unit_ids\"] = relationships_df[\"text_unit_ids\"].apply(lambda x: [i + f\"-{index_name}\" for i in x])\n",
    "        max_vals[\"relationships\"] = relationships_df[\"human_readable_id\"].astype(int).max()\n",
    "        relationships_dfs.append(relationships_df)\n",
    "    else:\n",
    "        print(f\"File {relationships_table_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(covariates_table_path):\n",
    "        covariates_df= pd.read_parquet(covariates_table_path)\n",
    "        if i in covariates_df[\"human_readable_id\"].astype(int):\n",
    "            links[\"covariates\"][i + max_vals[\"covariates\"] + 1] = {\"index_name\": index_name, \"id\": i}\n",
    "        if max_vals[\"covariates\"] != -1:\n",
    "            col = covariates_df[\"human_readable_id\"].astype(int) + max_vals[\"covariates\"] + 1\n",
    "            covariates_df[\"human_readable_id\"] = col.astype(str)\n",
    "        max_vals[\"covariates\"] = covariates_df[\"human_readable_id\"].astype(int).max()\n",
    "        covariates_dfs.append(covariates_df)\n",
    "    else:\n",
    "        print(f\"File {covariates_table_path} does not exist.\")\n",
    "        covariates_dfs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context_keys = ['reports', 'entities', 'relationships', 'claims', 'sources']\n",
    "def update_context(context, links):\n",
    "    updated_context = {}\n",
    "    for key in context:\n",
    "        updated_entry = []\n",
    "        if key == \"reports\":\n",
    "            updated_entry = [\n",
    "                dict(\n",
    "                    {k: entry[k] for k in entry},\n",
    "                    **{\n",
    "                        \"index_name\": links[\"community\"][int(entry[\"id\"])][\"index_name\"],\n",
    "                        \"index_id\": links[\"community\"][int(entry[\"id\"])][\"id\"],\n",
    "                    },\n",
    "                )\n",
    "                for entry in context[key]\n",
    "            ]\n",
    "        if key == \"entities\":\n",
    "            updated_entry = [\n",
    "                dict(\n",
    "                    {k: entry[k] for k in entry},\n",
    "                    **{\n",
    "                        \"entity\": entry[\"entity\"].split(\"-\")[0],\n",
    "                        \"index_name\": links[\"entities\"][int(entry[\"id\"])][\"index_name\"],\n",
    "                        \"index_id\": links[\"entities\"][int(entry[\"id\"])][\"id\"],\n",
    "                    },\n",
    "                )\n",
    "                for entry in context[key]\n",
    "            ]\n",
    "        if key == \"relationships\":\n",
    "            updated_entry = [\n",
    "                dict(\n",
    "                    {k: entry[k] for k in entry},\n",
    "                    **{\n",
    "                        \"source\": entry[\"source\"].split(\"-\")[0],\n",
    "                        \"target\": entry[\"target\"].split(\"-\")[0],\n",
    "                        \"index_name\": links[\"relationships\"][int(entry[\"id\"])][\"index_name\"],\n",
    "                        \"index_id\": links[\"relationships\"][int(entry[\"id\"])][\"id\"],\n",
    "                    },\n",
    "                )\n",
    "                for entry in context[key]\n",
    "            ]\n",
    "        if key == \"claims\":\n",
    "            updated_entry = [\n",
    "                dict(\n",
    "                    {k: entry[k] for k in entry},\n",
    "                    **{\n",
    "                        \"index_name\": links[\"claims\"][int(entry[\"id\"])][\"index_name\"],\n",
    "                        \"index_id\": links[\"claims\"][int(entry[\"id\"])][\"id\"],\n",
    "                    },\n",
    "                )\n",
    "                for entry in context[key]\n",
    "            ]\n",
    "        if key == \"sources\":\n",
    "            updated_entry = context[key]\n",
    "        updated_context[key] = updated_entry\n",
    "    return updated_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True)\n",
    "community_combined = pd.concat(community_dfs, axis=0, ignore_index=True)\n",
    "entities_combined = pd.concat(entities_dfs, axis=0, ignore_index=True)\n",
    "text_units_combined = pd.concat(text_units_dfs, axis=0, ignore_index=True)\n",
    "relationships_combined = pd.concat(relationships_dfs, axis=0, ignore_index=True)\n",
    "covariates_combined = pd.concat(covariates_dfs, axis=0, ignore_index=True) if covariates_dfs is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await global_search(config=parameters,\n",
    "    nodes=nodes_combined,\n",
    "    entities=entities_combined,\n",
    "    community_reports = community_combined,\n",
    "    community_level = 1,\n",
    "    response_type = \"Multiple Paragraphs\",\n",
    "    query= \"railroads in arizona\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = result[0]\n",
    "context = update_context(result[1], links)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p test5/output/00000000-000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await local_search(\n",
    "    root_dir = \"./test5\",\n",
    "    config=parameters,\n",
    "    nodes=nodes_combined,\n",
    "    entities=entities_combined,\n",
    "    community_reports = community_combined,\n",
    "    text_units = text_units_combined,\n",
    "    relationships = relationships_combined,\n",
    "    covariates = covariates_combined,\n",
    "    community_level = 1,\n",
    "    response_type = \"Multiple Paragraphs\",\n",
    "    query= \"railroads in arizona\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = result[0]\n",
    "context = update_context(result[1], links)\n",
    "context[\"reports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Single Index Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''nodes_df = pd.read_parquet('./arizona/output/20240820-192925/artifacts/create_final_nodes.parquet')\n",
    "entities_df = pd.read_parquet('./arizona/output/20240820-192925/artifacts/create_final_entities.parquet')\n",
    "community_df = pd.read_parquet('./arizona/output/20240820-192925/artifacts/create_final_community_reports.parquet')\n",
    "text_units_df = pd.read_parquet('./arizona/output/20240820-192925/artifacts/create_final_text_units.parquet')\n",
    "relationships_df = pd.read_parquet('./arizona/output/20240820-192925/artifacts/create_final_relationships.parquet')'''\n",
    "nodes_df = pd.read_parquet('./arizona/output/20240820-192210/artifacts/create_final_nodes.parquet')\n",
    "entities_df = pd.read_parquet('./arizona/output/20240820-192210/artifacts/create_final_entities.parquet')\n",
    "community_df = pd.read_parquet('./arizona/output/20240820-192210/artifacts/create_final_community_reports.parquet')\n",
    "text_units_df = pd.read_parquet('./arizona/output/20240820-192210/artifacts/create_final_text_units.parquet')\n",
    "relationships_df = pd.read_parquet('./arizona/output/20240820-192210/artifacts/create_final_relationships.parquet')\n",
    "covariates_df= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p single/output/00000000-000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await local_search(\n",
    "    root_dir = \"./single/\",\n",
    "    config=parameters,\n",
    "    nodes=nodes_df,\n",
    "    entities=entities_df,\n",
    "    community_reports = community_df,\n",
    "    text_units = text_units_df,\n",
    "    relationships = relationships_df,\n",
    "    covariates = covariates_df,\n",
    "    community_level = 1,\n",
    "    response_type = \"Multiple Paragraphs\",\n",
    "    query= \"where is alaska?\"\n",
    ")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await global_search(config=parameters,\n",
    "    nodes=nodes_df,\n",
    "    entities=entities_df,\n",
    "    community_reports = community_df,\n",
    "    community_level = 1,\n",
    "    response_type = \"Multiple Paragraphs\",\n",
    "    query= \"where is alaska?\"\n",
    ")\n",
    "print(result[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gracc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
