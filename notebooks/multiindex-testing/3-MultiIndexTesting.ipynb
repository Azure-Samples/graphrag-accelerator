{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Index Testing with graphrag library API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import query api from graphrag library\n",
    "# Make sure to install graphrag version 0.3.0 to be able to get these packages\n",
    "from graphrag.query.api import global_search, local_search\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "import yaml\n",
    "from graphrag.config import create_graphrag_config\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom pipeline settings m\n",
    "this_directory = os.path.dirname(\n",
    "            os.path.abspath(inspect.getfile(inspect.currentframe()))\n",
    "        )\n",
    "data = yaml.safe_load(open(f\"./pipeline-settings.yaml\"))\n",
    "# layer the custom settings on top of the default configuration settings of graphrag\n",
    "parameters = create_graphrag_config(data, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logic to iterate through indexes, convert parquets to dataframes, append dataframes to a list, then concat them together to pass to graphrag.query.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index folder names\n",
    "\n",
    "index_names = ['./arizona', './alaska']\n",
    "\n",
    "report_dfs = []\n",
    "nodes_dfs =[]\n",
    "community_dfs =[]\n",
    "entities_dfs = []\n",
    "text_units_dfs=[]\n",
    "relationships_dfs = []\n",
    "covariates_dfs = []\n",
    "\n",
    "for index_name in index_names:\n",
    "# Construct the path to the output directory\n",
    "    output_path = os.path.join(index_name, \"output\")\n",
    "    \n",
    "    # Get all subdirectories in the output folder\n",
    "    subdirs = [os.path.join(output_path, d) for d in os.listdir(output_path) if os.path.isdir(os.path.join(output_path, d))]\n",
    "    \n",
    "    # Find the most recently created subdirectory\n",
    "    most_recent_subdir = max(subdirs, key=os.path.getmtime)\n",
    "    \n",
    "    # Construct the path to the respective parquet files, takes the most recent index iteration in the artifacts folder\n",
    "    nodes_file_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_nodes.parquet\")\n",
    "    community_report_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_community_reports.parquet\")  # Adjust the filename/path as necessary\n",
    "    entities_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_entities.parquet\")  # Adjust the filename/path as necessary\n",
    "    text_units_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_text_units.parquet\")\n",
    "    relationships_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_relationships.parquet\")\n",
    "    covariates_table_path = os.path.join(most_recent_subdir, \"artifacts\", \"create_final_covariates.parquet\")\n",
    "    \n",
    "    # Read the parquet file into a DataFrame\n",
    "    if os.path.exists(nodes_file_path):\n",
    "        nodes_df = pd.read_parquet(nodes_file_path)\n",
    "        nodes_dfs.append(nodes_df)\n",
    "    else:\n",
    "        print(f\"File {nodes_file_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(community_report_table_path):\n",
    "        community_df= pd.read_parquet(community_report_table_path)\n",
    "        community_dfs.append(community_df)\n",
    "    else:\n",
    "        print(f\"File {community_report_table_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(entities_table_path):\n",
    "        entities_df= pd.read_parquet(entities_table_path)\n",
    "        entities_dfs.append(entities_df)\n",
    "    else:\n",
    "        print(f\"File {entities_table_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(text_units_table_path):\n",
    "        text_units_df= pd.read_parquet(text_units_table_path)\n",
    "        text_units_dfs.append(text_units_df)\n",
    "    else:\n",
    "        print(f\"File {text_units_table_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(relationships_table_path):\n",
    "        relationships_df= pd.read_parquet(relationships_table_path)\n",
    "        relationships_dfs.append(relationships_df)\n",
    "    else:\n",
    "        print(f\"File {relationships_table_path} does not exist.\")\n",
    "    \n",
    "    if os.path.exists(covariates_table_path):\n",
    "        covariates_df= pd.read_parquet(covariates_table_path)\n",
    "        covariates_dfs.append(covariates_df)\n",
    "    else:\n",
    "        print(f\"File {covariates_table_path} does not exist.\")\n",
    "        covariates_dfs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True)\n",
    "community_combined = pd.concat(community_dfs, axis=0, ignore_index=True)\n",
    "entities_combined = pd.concat(entities_dfs, axis=0, ignore_index=True)\n",
    "text_units_combined = pd.concat(text_units_dfs, axis=0, ignore_index=True)\n",
    "relationships_combined = pd.concat(relationships_dfs, axis=0, ignore_index=True)\n",
    "covariates_combined = pd.concat(covariates_dfs, axis=0, ignore_index=True) if covariates_dfs is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await global_search(config=parameters,\n",
    "              nodes=nodes_combined,\n",
    "              entities=entities_combined,\n",
    "              community_reports = community_combined,\n",
    "              community_level = 1,\n",
    "              response_type = \"Multiple Paragraphs\",\n",
    "              query= \"where is arizona?\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await local_search(config=parameters,\n",
    "              nodes=nodes_combined,\n",
    "              entities=entities_combined,\n",
    "              community_reports = community_combined,\n",
    "              text_units = text_units_combined,\n",
    "              relationships = relationships_combined,\n",
    "              covariates = covariates_combined,\n",
    "              community_level = 1,\n",
    "              response_type = \"Multiple Paragraphs\",\n",
    "              query= \"where is arizona?\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Single Index Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = pd.read_parquet('./alaska/output/20240820-192925/artifacts/create_final_nodes.parquet')\n",
    "entities_df = pd.read_parquet('./alaska/output/20240820-192925/artifacts/create_final_entities.parquet')\n",
    "community_df = pd.read_parquet('./alaska/output/20240820-192925/artifacts/create_final_community_reports.parquet')\n",
    "text_units_df = pd.read_parquet('./alaska/output/20240820-192925/artifacts/create_final_text_units.parquet')\n",
    "relationships_df = pd.read_parquet('./alaska/output/20240820-192925/artifacts/create_final_relationships.parquet')\n",
    "covariates_df= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await local_search(config=parameters,\n",
    "              nodes=nodes_df,\n",
    "              entities=entities_df,\n",
    "              community_reports = community_df,\n",
    "              text_units = text_units_df,\n",
    "              relationships = relationships_df,\n",
    "              covariates = covariates_df,\n",
    "              community_level = 1,\n",
    "              response_type = \"Multiple Paragraphs\",\n",
    "              query= \"where is alaska?\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await global_search(config=parameters,\n",
    "              nodes=nodes_df,\n",
    "              entities=entities_df,\n",
    "              community_reports = community_df,\n",
    "              community_level = 1,\n",
    "              response_type = \"Multiple Paragraphs\",\n",
    "              query= \"where is alaska?\"\n",
    "              )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gracc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
